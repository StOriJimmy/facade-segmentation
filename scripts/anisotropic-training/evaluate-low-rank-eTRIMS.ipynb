{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the results of anisotropic training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the eTRIMS data to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import zipfile\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def download(url, ):\n",
    "    chunksize=2**20\n",
    "    output_path = os.path.basename(url)\n",
    "    with open(output_path, 'wb') as f:\n",
    "        g = urllib2.urlopen(url) \n",
    "        total = int(g.info().getheaders(\"Content-Length\")[0])\n",
    "        downloaded = 0\n",
    "        while True:\n",
    "            chunk = g.read(chunksize)\n",
    "            f.write(chunk)\n",
    "            downloaded += len(chunk)\n",
    "            sys.stdout.write('\\r {:3.2%} Read {:>16} of {:<16} bytes'.format( downloaded /float(total), downloaded, total))\n",
    "            sys.stdout.flush()\n",
    "            if len(chunk) < chunksize:\n",
    "                break\n",
    "    sys.stdout.write('\\n')\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ETRIMS_URL_PATTERN = 'http://www.ipb.uni-bonn.de/projects/etrims_db/downloads/etrims-db_{}.zip'\n",
    "ETRIMS_URLS= [ETRIMS_URL_PATTERN.format(n) for n in 'v1','beta1', 'beta2']\n",
    "\n",
    "# !mkdir -p etrims\n",
    "# zipfile.ZipFile(download(ETRIMS_URLS[0])).extractall('etrims')\n",
    "# zipfile.ZipFile(download(ETRIMS_URLS[1])).extractall('etrims')\n",
    "# zipfile.ZipFile(download(ETRIMS_URLS[2])).extractall('etrims')\n",
    "# !rm etrims-db_beta2.zip   \n",
    "# !rm etrims-db_v1.zip\n",
    "# !rm etrims-db_beta1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "etrims_images = glob('etrims/etrims-db_v1/images/08*/*.jpg')\n",
    "etrims_labels = [fn.replace('/images/', '/annotations/').replace('.jpg','.png') for fn in etrims_images]\n",
    "print len(etrims_images), \"images from etrims\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyfacades.util.metrics import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CPU = True\n",
    "import caffe\n",
    "if CPU:\n",
    "    caffe.set_mode_cpu()\n",
    "else:\n",
    "    caffe.set_mode_gpu()\n",
    "    caffe.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PROTO = 'non-bayesian-inference-net.prototxt'\n",
    "#WEIGHTS = 'deploy/test_weights.caffemodel'\n",
    "WEIGHTS = 'test_weights_from_peihao.caffemodel'\n",
    "net = caffe.Net(PROTO, WEIGHTS, caffe.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the batch size to one\n",
    "net.blobs['data'].reshape(1, 3, 512, 512)\n",
    "net.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NEG = 0\n",
    "UNK = 1\n",
    "POS = 2\n",
    "EDG = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyfacades.util import split_tiles, combine_tiles, softmax, channels_first, channels_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyfacades.rectify import Homography as AffaraRectifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyfacades.models.driving_12x360x480 import process_strip as segment_driving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage.transform import warp, ProjectiveTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage.morphology import binary_dilation, binary_erosion, disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Tracer\n",
    "set_trace = Tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run -i color_coded_errors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_metrics(f, vis=False, rectify=True):\n",
    "    # Read RGB, Labels from eTRIMS\n",
    "    idx = etrims_images.index(f)\n",
    "    rgb = skimage.io.imread(etrims_images[idx])\n",
    "    labels = skimage.io.imread(etrims_labels[idx])\n",
    "    \n",
    "    valid = np.ones(rgb.shape[:2])\n",
    "    valid=np.pad(valid, ((0, 0), (150, 150)), mode='constant')    \n",
    "    rgb=np.pad(rgb, ((0, 0), (150, 150), (0,0)), mode='constant')\n",
    "    labels=np.pad(labels, ((0, 0), (150, 150), (0,0)), mode='constant')\n",
    "    \n",
    "    expected_windows = (labels == array([0,0,128])).all(2).astype(int)   \n",
    "    se = disk(2)\n",
    "    edges = binary_dilation(expected_windows, selem=se) & ~binary_erosion(expected_windows, selem=se)\n",
    "    expected_windows[edges] = 2\n",
    "    \n",
    "    # Rectify\n",
    "    presegment = segment_driving(channels_first(rgb))\n",
    "    mask = binary_erosion(valid>0, selem=disk(3)) & (presegment.building() > 0.5)\n",
    "    rectifier = AffaraRectifier(rgb, mask=mask)\n",
    "\n",
    "    if rectify:    \n",
    "        rectified_rgb = rectifier.rectified\n",
    "        rectified_mask = rectifier.rectified_mask\n",
    "        rectified_labels = warp(labels, ProjectiveTransform(rectifier.H), preserve_range=True).astype(np.uint8)\n",
    "    else:\n",
    "        rectified_rgb = rgb/255.\n",
    "        rectified_mask = mask\n",
    "        rectified_labels = labels\n",
    "        \n",
    "    rectified_windows = (rectified_labels == array([0,0,128])).all(2)\n",
    "\n",
    "    # Predict / Inference\n",
    "    tiles = list(split_tiles(channels_first(rectified_rgb), (512, 512)))\n",
    "    prob_tiles = []\n",
    "    for i, tile in enumerate(tiles):\n",
    "        result = net.forward(data=np.array([tile*255]), blobs=['conv-window', 'conv-shop']) \n",
    "        prob_window = softmax(result['conv-window'][0,(0,2)])\n",
    "        prob_shop = softmax(result['conv-shop'][0,(0,2)])\n",
    "        \n",
    "        prob_tiles.append(array([prob_window[0], prob_window[1], prob_shop[1]])) # + result['prob-shop'][0] )    \n",
    "    rectified_prediction = combine_tiles(array(prob_tiles), (3,)+labels.shape[:2])\n",
    "    \n",
    "    # Un-Rectify for comparison\n",
    "    if rectify:\n",
    "        prediction =  channels_first(warp(channels_last(rectified_prediction), ProjectiveTransform(rectifier.inv_H)))\n",
    "    else:\n",
    "        prediction = rectified_prediction\n",
    "    predicted_windows = prediction.argmax(0)\n",
    "    #predicted_windows[~mask] = 0\n",
    "\n",
    "    mf = Metrics(expected=expected_windows, \n",
    "             predicted=predicted_windows, \n",
    "             label_positive=1,\n",
    "             label_negative=0,\n",
    "             source=f,\n",
    "             feature='window'\n",
    "            )\n",
    "    if vis:\n",
    "        clf()\n",
    "        subplot2grid((3, 5), (0,0))\n",
    "        imshow(rgb)\n",
    "        title('RGB')\n",
    "        axis('off')\n",
    "        subplot2grid((3, 5), (0,1))\n",
    "        title('Labels')\n",
    "        imshow(labels)\n",
    "        axis('off')\n",
    "        subplot2grid((3, 5), (1,0))\n",
    "        title('Windows')\n",
    "        imshow(expected_windows, vmin=0, vmax=2)\n",
    "        axis('off')\n",
    "        subplot2grid((3, 5), (1,1))\n",
    "        title('Pred. Windows')\n",
    "        imshow(predicted_windows, vmin=0, vmax=2)\n",
    "        axis('off')\n",
    "        subplot2grid((3, 5), (2,0))\n",
    "        title('Rectified')\n",
    "        rectifier.plot_rectified()\n",
    "        title(None)\n",
    "        axis('off')\n",
    "        subplot2grid((3, 5), (2,1))\n",
    "        title('Probs')\n",
    "        imshow(prediction[1], vmin=0, vmax=1, cmap=cm.gray)\n",
    "        axis('off')\n",
    "        \n",
    "        # Render the color coded errors in a large subplot on the right\n",
    "        subplot2grid((3, 5), (0,2), colspan=3, rowspan=3)\n",
    "        alpha = 0.5\n",
    "        cc = color_coded_errors(expected_windows==1, predicted_windows==1, \n",
    "                                (expected_windows==2) | (predicted_windows==2))\n",
    "        #set_trace()\n",
    "        rgb[~cc.mask] = (1-alpha)*rgb[~cc.mask] + alpha*cc[~cc.mask]\n",
    "        rgb = rgb.clip(0,255)\n",
    "        imshow(rgb)\n",
    "        axis('off')\n",
    "        \n",
    "        suptitle('A:{}, P:{}, R:{}'.format(mf.pixel_accuracy, mf.pixel_precision, mf.pixel_recall))\n",
    "\n",
    "    return mf, channels_first(rgb), expected_windows, predicted_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(9,8))\n",
    "mf, rgb, expected, predicted = get_metrics(etrims_images[12], vis=True, rectify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import anydbm\n",
    "import json\n",
    "import hashlib\n",
    "import munch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_results = anydbm.open('eval_low_rank_peihao_etrims', 'c')\n",
    "checksum = hashlib.md5(open(WEIGHTS).read()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'md5' in eval_results and eval_results['md5'] == checksum:\n",
    "    print \"We already seem to have run evaluation...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(etrims_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in  etrims_images:\n",
    "    try:\n",
    "        json.loads(eval_results[f])\n",
    "    except:\n",
    "        print \"Removing invalid\", f\n",
    "        del eval_results[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p lr_etrims_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pdb on\n",
    "recompute = True\n",
    "visualize = True\n",
    "\n",
    "if visualize:\n",
    "    fig = figure(figsize=(12,8))\n",
    "\n",
    "total = Metrics(feature='windows')\n",
    "for i, f in enumerate(etrims_images):   \n",
    "    if recompute or f not in eval_results:\n",
    "        mf, rgb, expected, predicted = get_metrics(f, visualize)\n",
    "        eval_results[f] = json.dumps(mf.as_dict())\n",
    "        if visualize:\n",
    "            try:\n",
    "                suptitle('{} of {}, $P$:{:.2%}, $R$:{:.2%}, $F_1$:{:.2%}, $A$:{:.2%}'.format(i, len(etrims_images), total.pixel_precision, total.pixel_recall, total.pixel_f1, total.pixel_accuracy))\n",
    "            except ZeroDivisionError:\n",
    "                suptitle(\"Not enough samples yet....\")\n",
    "                \n",
    "            fig.canvas.draw()\n",
    "            stem = os.path.splitext(os.path.basename(f))[0]\n",
    "            savefig('lr_etrims_2/{}.png'.format(stem))\n",
    "    else:\n",
    "        mf = Metrics(**json.loads(eval_results[f]))\n",
    "    #print mf, \n",
    "    total += mf\n",
    "    print '\\r{}  of {}: '.format(i, len(etrims_images)), total,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is recall so bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run -i color_coded_errors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score(f):\n",
    "    acc = Metrics(**json.loads(eval_results[f])).pixel_f1\n",
    "    if isnan(acc):\n",
    "        acc = 0\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accs = array([score(f) for f in etrims_images])\n",
    "ranking = argsort(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "plot(accs[ranking]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the best 9 images as examples to compare with other methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comparison_files = [etrims_images[ranking[-i-1]] for i in range(9)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('eTRIMS_comparison_files.txt', 'w') as f:\n",
    "    f.writelines([fn + '\\n' for fn in comparison_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = figure(figsize=(9,9))\n",
    "plt.subplots_adjust(wspace=0)\n",
    "for i in range(9):\n",
    "    subplot(3,3,i+1)\n",
    "    cached = 'separable-eTRIMS-top-{}.png'.format(i+1)\n",
    "    if os.path.isfile(cached):\n",
    "        err_image = imread(cached)\n",
    "    else:\n",
    "        err_image = render_errors(comparison_files[i], alpha=0.6);\n",
    "        imsave(cached, err_image)\n",
    "    imshow(err_image)\n",
    "    xticks([]); yticks([]); #xlabel(cached, fontsize=8)\n",
    "    fig.canvas.draw()\n",
    "fig.tight_layout()\n",
    "savefig('separable-eTRIMS-top-9-figure.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell I sort the images by increasing $F_1$ score. The worst images have the smallest index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracies = [Metrics(**json.loads(eval_results[f])).pixel_f1 for f in etrims_images]\n",
    "argworst = argsort(accuracies)\n",
    "worst = array(etrims_images)[argworst]\n",
    "worst_indices = [etrims_images.index(k) for k in worst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(8, 8))\n",
    "result = get_metrics(worst[-1], vis=True)\n",
    "subplot(321); title('original')\n",
    "subplot(322); title('label-colors')\n",
    "subplot(323); title('expected')\n",
    "subplot(324); title('predicted')\n",
    "subplot(325); title('rectified')\n",
    "subplot(326); title('$\\Pr[\\text{window}]$')\n",
    "savefig('worst_example.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(8, 16))\n",
    "result = get_metrics(worst[0], vis=True)\n",
    "subplot(321); title('original')\n",
    "subplot(322); title('label-colors')\n",
    "subplot(323); title('expected')\n",
    "subplot(324); title('predicted')\n",
    "subplot(325); title('rectified')\n",
    "subplot(326); title(r'Pr[window]')\n",
    "savefig('worst_example.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(4, 8))\n",
    "result = get_metrics(worst[1], vis=True)\n",
    "figure(figsize=(4, 8))\n",
    "result = get_metrics(worst[2], vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir lr-etrims\n",
    "fig = figure(figsize=(8, 8))\n",
    "for i, fn in enumerate(worst):\n",
    "    fig.clf()\n",
    "    result = get_metrics(fn, vis=True)\n",
    "    m  = result[0]\n",
    "    subplot(321); title('original')\n",
    "    subplot(322); title('label-colors')\n",
    "    subplot(323); title('expected')\n",
    "    subplot(324); title('predicted')\n",
    "    subplot(325); title('rectified')\n",
    "    subplot(326); title('$\\Pr[\\text{window}]$')\n",
    "    title('A:{:.2f}, P:{:.2f}, R:{:.2f}, F:{:.2f}'.format(m.pixel_accuracy, m.pixel_precision, m.pixel_recall, m.pixel_f1))\n",
    "    savefig('lr-etrims/{:03}.png'.format(i), dpi=300)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from munch import Munch, munchify, toYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_metrics = Munch()\n",
    "all_metrics.files = Munch()\n",
    "total = Metrics()\n",
    "for fn in etrims_images:\n",
    "    stem = os.path.splitext(os.path.basename(fn))[0]\n",
    "    metrics =  Metrics(**json.loads(eval_results[fn]))\n",
    "    total += metrics\n",
    "    all_metrics.files[stem] = munchify(metrics.as_dict())\n",
    "all_metrics.total = munchify(total.as_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print all_metrics.total.pixel_f1\n",
    "print all_metrics.total.pixel_recall\n",
    "print all_metrics.total.pixel_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('low_rank_etrims_attempt_one.yml', 'w') as f:\n",
    "    f.write(toYAML(all_metrics))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def reverse_engineer_acc(P, R):\n",
    "    pos = 5988721\n",
    "    neg = 57149895\n",
    "    TP = R*pos\n",
    "    FN = pos - TP\n",
    "    FP = TP*(1.-P)/P\n",
    "    TN = neg-FP\n",
    "    acc = (TP+TN)/(TP+FP+TN+FN)\n",
    "    return acc\n",
    "\n",
    "print '{:.1%}'.format(reverse_engineer_acc(0.73, 0.62))\n",
    "print '{:.1%}'.format(reverse_engineer_acc(0.95, 0.69))\n",
    "print '{:.1%}'.format(reverse_engineer_acc(0.91, 0.71))\n",
    "\n",
    "# 94.2%\n",
    "# 96.7%\n",
    "# 96.6%"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "cmp_total = Metrics()\n",
    "for f in eval_results:\n",
    "    if '/cmp/' in f:\n",
    "        print  f\n",
    "        cmp_total += Metrics(**json.loads(eval_results[f]))\n",
    "print cmp_total, 'Acc:', cmp_total.pixel_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:femiani]",
   "language": "python",
   "name": "conda-env-femiani-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
